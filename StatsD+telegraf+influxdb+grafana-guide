Implementation of Actuator using statsD 
1.Add metrics-actuator dependencies.
2.Add to application properties management.endpoints.web.exposure.include=* to expose all the data metrics at endpoint. 
3.We can create our customized end point by extending predefined.
4.We can define end points using annotations @WriteOperation at put api, @ReadOperation at get api and @DeleteOperation at delete api.
5. Now run your application and check at /actuator/info or /mappings or /metrics or /env to view various actuator data.

Adding Statsd

6.Add Micrometer Configuration : create a bean for meterRegistryCustomizer with meterRegistry .
7.Add @Timed annotation at your controller or at ur api's for different metrics.
@Timed(value="userInfo.gettingAll.request",
		       histogram=true,
		       percentiles = {0.95,0.99},
		       extraTags = {"version","1.0"})
We can add extra tags , histograms to maintain history, percentiles, extra tags, and long task if task is long.
8.Now Add to application properties the statsd port number where u want send statsd data.
management.metrics.export.statsd.port:9125, by default port number is 9125

9.Now after running application we can check actuator/metric and after api call we can see the timed metrics columns getting created.

Adding telegrad and influxdb to store data.
10.We need to download influxdb and telegraf onto our system.
11.Run the telegraf on your system check its status
12 using command sudo systemctl status influxdb
13.now create a database in influxdb with name telegraf and then create a user with password and allow privileges.
14.Now create a telegraf.conf file with a input source statsd and output source influxdb using command :  telegraf -sample-config -input-filter statsd -output-filter influxdb > telegraf.conf

15.Now open telegraf.conf in edit mode using cmd:
gedit  /etc/telegraf/telegraf.conf.

16.Now add the user name and password and database name of ur influxdb database;
17. then close and run ur telegraf using cmd: telegraf -debug
18.Now run ur application ,if there is error u will get to know about it.
19.Now check ur database if data is getting stored.

Now Adding Grafana to view graphically your collected metrics.
20.Download Grafana.
21.run it in localhost:3000
22.check all the ports running using cmd:sudo netstat -tulpn
23.We can kill port using cmd : sudo kill -9 portId
24.Now we open grafana and add the database.
25.Select database and provide basic auth username and password and choose direct connection.
26.After that u can add new Components and panels on ur grafana .
27. Edit the panels and add new queries to make visualize data.



Now Adding Restrictions on Api's
1.Create a @Annotation Throttle which take various inputs like second,minutes,hours and day request.
2.Create Aspect with conditions to check and store the api logs and throw error if logs exceeds limits.
3.Now create a jar of this application.
4.Add dependency in you Api and use it.
5.you need to define all 4 properties in ur application.properties so it dont cause bean creation error.
6.Successfully Run your application.


Now Adding Email Error Sending:
1.Create a microservice to send in your api.Also create restTemplate bean.
2.Add a template in your email api to send.
3.Now To send email at every request exceeds. 
4.Add email api caller using requestTemplate at condition of throttle aspect.


Implementing new Relic
1.Create an account on rpm.newrelic.com.
2.Download the latest java agent.Currently 5.11.0
3.Extract files on to your Resource. Using cmd unzip newrelic-java-5.11.0.zip -d /path/to/resource .
4.Add newrelic agent dependencies in pom file.
5.Edit the newrelif.yml file.
6.Add your license key and application name in newrelif.yml.
7.Now copy the newrelif.jar and newrelif.yml in your application target folder
8.Now run using cmd :java -javaagent:newrelic.jar -jar myapplication.jar 
9.Agent at the newrelic will be created .
10.Now we can view various api logs and visualize api .


Splunk vs newRelic 
Similarities:

    Initially focused on machine data (metrics for NR, logs for Splunk)
    Initially focused on DevOps, but spreading to other areas - security, marketing, business metrics, and so on
    Both fairly expensive compared to most of their alternatives

Differences:

    New Relic is focused on APM at the moment. Splunk is/was focused on logs and other forms of event data, other than metrics
    Splunk is focused on On Premises deployments (they have/had a Cloud offering, but it is/was pretty weak), while New Relic is Cloud-only
    New Relic was initially focused on small businesses and is growing upwards from there, while Splunk is (still) focused on large enterprises

Alternatives:

Both New Relic and Splunk have alternatives:

    For New Relic alternative you could go for AppDynamics, which is powerful but pricey, or Datadog or SPM from us at Sematext, which are much more affordable. Datadog is Cloud-only, while SPM is both Cloud and On Premises.
    For Splunk alternative you could try DIY ELK stack (free software, but not free time/labour/expertise needed) or something like Logsene from Sematext, which gives you Elasticsearch API, Kibana, etc. and can be used in the Cloud or On Premises.


Implementing MongoDB.
1.Download MongoDb : sudo apt-get install -y mongodb-org
2.Start and check status of mongoDB using cmd :sudo systemctl start mongod
sudo systemctl enable mongod
3.Now open mongodb using cmd :mongo
4.Add a user with name and password and role, using create user in mongoDB.
5.Now create a model in spring boot and add @Document .
6.Each row in mongoDb is document , each table is a collection, and above that is database.
7.Create a Repository interface and extend it to MongoRepository . Now we can create MongoTemplate and MongoRepository methods based on our requirement.
8.MongoTemplates provide better searching methods and MongoRepository provides CRUD operations.
9.Now Create a service and controller , Add to application properties, MongoDb port,username,password,dbname.
10.spring.data.mongodb.host=localhost
spring.data.mongodb.port=27017
spring.data.mongodb.database=admin
spring.data.mongodb.username=admin
spring.data.mongodb.password=admin
spring.data.mongodb.auto-index-creation=false
11.We can also add it like this, by default database is test. But We can change name and it will be autocreated.
#spring.data.mongodb.uri=mongodb://root:root@localhost:27017/test_db



Implementing MongoDB aggregation functions
1.We can implement various mongoDB aggregations using :MongodbTemplate class and using aggregate with further queries.
2.It works in stages : we can match data then group them and perform more operations and vice-versa .
3.Various aggregate functions are :Min,max,sum,avg,first,last,addToSet, push.
4.Further we have pipelines : pipeline means the possibility to execute an operation on some input and use the output as the input for the next command and so on.
Various pipeline Functions are:
1.$project − Used to select some specific fields from a collection.
2.$match − This is a filtering operation and thus this can reduce the amount of documents that are given as input to the next stage.
3.$group − This does the actual aggregation as discussed above.
4.$sort − Sorts the documents.
5.$skip − With this, it is possible to skip forward in the list of documents for a given amount of documents.
6.$limit − This limits the amount of documents to look at, by the given number starting from the current positions.
7.$unwind −To expand doc again.


MongoDb:Replication
It making replication of your database to use it when one of the database fails.
1.mkdir -p rs1 rs2 rs3  -creating directories
2.mongod --replSet dhruv --logpath "1.log" --dbpath --port 27017 & 
  mongod --replSet dhruv --logpath "2.log" --dbpath --port 27018 &
  mongod --replSet dhruv --logpath "3.log" --dbpath --port 27019 &

 to run locally on different port and different log.
3.Add config in mongodb
 config={_id:"dhruv",members:[
{_id:0,host : "localhost:27017"},
{_id:1,host : "localhost:27018"},
{_id:2,host : "localhost:27019"},
]};

4. rs.initiate(config);
5.rs.status();
6.check running mongo using cmd: ps -ef | grep mongod
7. Now kill primary using kill code.
8. mongo --host dhruv/localhost:27017,localhost:27018,localhost:27019
The replica set is a group of mongod instances that maintain the same data set.
This example shows how to configure a replica set with three instances on the same server.
Creating data folders
mkdir /srv/mongodb/data/rs0-0
mkdir /srv/mongodb/data/rs0-1
mkdir /srv/mongodb/data/rs0-2
Starting mongod instances
mongod --port 27017 --dbpath /srv/mongodb/data/rs0-0 --replSet rs0
mongod --port 27018 --dbpath /srv/mongodb/data/rs0-1 --replSet rs0
mongod --port 27019 --dbpath /srv/mongodb/data/rs0-2 --replSet rs0
Configuring replica set
mongo --port 27017
// connection to the instance 27017
rs.initiate();
rs.add("<hostname>:27018")
rs.add("<hostname>:27019")
// initilization of replica set on the 1st node
// adding a 2nd node
// adding a 3rd node
Testing your setup
For checking the configuration type rs.status()




MongoDb sharding: 
Sharding is the process of distributing data across multiple servers for storage.
By default there are 3 config servers.
Sharding Group Members :
For sharding there are three players.
1. Config Server
2. Replica Sets
3. Mongos
For a mongo shard we need to setup the above three servers.
Config Server Setup : add the following to mongod conf file
sharding:
clusterRole: configsvr
replication:
replSetName: <setname>
run : mongod --config
we can choose config server as replica set or may be a standalone server. Based on our requirement we can choose the
best. If config need to run in replica set we need to follow the replica set setup
Replica Setup : Create replica set // Please refer the replica setup
MongoS Setup : Mongos is main setup in shard. Its is query router to access all replica sets
Add the following in mongos conf file
sharding:
configDB: <configReplSetName>/cfg1.example.net:27017;
Configure Shared :
Connect the mongos via shell (mongo --host --port )
1. sh.addShard( "/s1-mongo1.example.net:27017")
2. sh.enableSharding("")
3. sh.shardCollection("< database >.< collection >", { < key > : < direction > } )
4. sh.status() // To ensure the sharding


MongoDb :
(Push:- to add new field ,       db.people.update({name: 'Tom'}, {$push: {nicknames: 'Tommy'}})
 Pop:- to delete a element in a array,-1 for 1st element, 1 for last element ,
 pull:-to delete a string in a array,   
 sort -1 for descending  and 1 for ascending        
 Poll, Array, MultiUpdate, Find and Modify, Query with sort)








