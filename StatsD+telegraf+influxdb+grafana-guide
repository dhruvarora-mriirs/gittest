Spring Boot Actuator is a sub-project of the Spring Boot Framework. It includes a number of additional features that help us to monitor and manage the Spring Boot application. It contains the actuator endpoints (the place where the resources live). We can use HTTP and JMX endpoints to manage and monitor the Spring Boot application. 

Metrics: Spring Boot Actuator provides dimensional metrics by integrating with the micrometer. The micrometer is integrated into Spring Boot. It is the instrumentation library powering the delivery of application metrics from Spring. It provides vendor-neutral interfaces for timers, gauges, counters, distribution summaries, and long task timers with a dimensional data model.

Implementation of Actuator using statsD 
1.Add metrics-actuator dependencies.
2.Add to application properties management.endpoints.web.exposure.include=* to expose all the data metrics at endpoint. 
3.We can create our customized end point by extending predefined.
4.We can define end points using annotations @WriteOperation at put api, @ReadOperation at get api and @DeleteOperation at delete api.
5. Now run your application and check at /actuator/info or /mappings or /metrics or /env to view various actuator data.

The StatsD registry pushes metrics over UDP to a StatsD agent eagerly. By default, metrics are exported to a StatsD agent running on your local machine. 
Adding Statsd

6.Add Micrometer Configuration : create a bean for meterRegistryCustomizer with meterRegistry .
7.Add @Timed annotation at your controller or at ur api's for different metrics.
@Timed(value="userInfo.gettingAll.request",
		       histogram=true,
		       percentiles = {0.95,0.99},
		       extraTags = {"version","1.0"})
We can add extra tags , histograms to maintain history, percentiles, extra tags, and long task if task is long.
8.Now Add to application properties the statsd port number where u want send statsd data.
management.metrics.export.statsd.port:9125, by default port number is 9125

9.Now after running application we can check actuator/metric and after api call we can see the timed metrics columns getting created.

TELEGRAF
Telegraf is a plugin-driven server agent for collecting and sending metrics and events from databases, systems, and IoT sensors.
Telegraf is written in Go and compiles into a single binary with no external dependencies, and requires a very minimal memory footprint.

InfluxDB is an open-source time series database (TSDB) developed by InfluxData. It is written in Go and optimized for fast, high-availability storage and retrieval of time series data in fields such as operations monitoring, application metrics, Internet of Things sensor data, and real-time analytics. It also has support for processing data from Graphite.

Adding telegrad and influxdb to store data.
10.We need to download influxdb and telegraf onto our system.
11.Run the telegraf on your system check its status
12 using command sudo systemctl status influxdb
13.now create a database in influxdb with name telegraf and then create a user with password and allow privileges.
14.Now create a telegraf.conf file with a input source statsd and output source influxdb using command :  telegraf -sample-config -input-filter statsd -output-filter influxdb > telegraf.conf

15.Now open telegraf.conf in edit mode using cmd:
gedit  /etc/telegraf/telegraf.conf.

16.Now add the user name and password and database name of ur influxdb database;
17. then close and run ur telegraf using cmd: telegraf -debug
18.Now run ur application ,if there is error u will get to know about it.
19.Now check ur database if data is getting stored.

Grafana is a general purpose dashboard and graph composer. It's focused on providing rich ways to visualize time series metrics, mainly though graphs but supports other ways to visualize data through a pluggable panel architecture. It currently has rich support for for Graphite, InfluxDB and OpenTSDB. But supports other data sources via plugins.

Now Adding Grafana to view graphically your collected metrics.
20.Download Grafana.
21.run it in localhost:3000
22.check all the ports running using cmd:sudo netstat -tulpn
23.We can kill port using cmd : sudo kill -9 portId
24.Now we open grafana and add the database.
25.Select database and provide basic auth username and password and choose direct connection.
26.After that u can add new Components and panels on ur grafana .
27. Edit the panels and add new queries to make visualize data.



Now Adding Restrictions on Api's
1.Create a @Annotation Throttle which take various inputs like second,minutes,hours and day request.
2.Create Aspect with conditions to check and store the api logs and throw error if logs exceeds limits.
3.Now create a jar of this application.
4.Add dependency in you Api and use it.
5.you need to define all 4 properties in ur application.properties so it dont cause bean creation error.
6.Successfully Run your application.


Now Adding Email Error Sending:
1.Create a microservice to send in your api.Also create restTemplate bean.
2.Add a template in your email api to send.
3.Now To send email at every request exceeds. 
4.Add email api caller using requestTemplate at condition of throttle aspect.


Implementing new Relic
1.Create an account on rpm.newrelic.com.
2.Download the latest java agent.Currently 5.11.0
3.Extract files on to your Resource. Using cmd unzip newrelic-java-5.11.0.zip -d /path/to/resource .
4.Add newrelic agent dependencies in pom file.
5.Edit the newrelif.yml file.
6.Add your license key and application name in newrelif.yml.
7.Now copy the newrelif.jar and newrelif.yml in your application target folder
8.Now run using cmd :java -javaagent:newrelic.jar -jar myapplication.jar 
9.Agent at the newrelic will be created .
10.Now we can view various api logs and visualize api .






NEW RELIC
New Relic is the all-in-one web application performance tool that lets you see performance from the end user experience, through servers, and down to the line of application code.
New Relic works through agents and provides Application monitoring,Server Monitoring,Browser Monitoring,Mobile and availability Monitoring.

SPLUNK
Splunk is a machine monitoring tool:it uses forwarders to collect and send data to splunk , then indexers to store data ,then visualizers to visualize data.
It help the user to view : System performance , failure Conditions , business matrix, search and investigate , visualize and store.
Benefits
Can input data in any form, searches using spl, not a single failure and directly store data into splunk file system, fast scalability and installation.

Splunk vs newRelic 
Similarities:

    Initially focused on machine data (metrics for NR, logs for Splunk)
    Initially focused on DevOps, but spreading to other areas - security, marketing, business metrics, and so on
    Both fairly expensive compared to most of their alternatives

Differences:

    New Relic is focused on APM at the moment. Splunk is/was focused on logs and other forms of event data, other than metrics
    Splunk is focused on On Premises deployments (they have/had a Cloud offering, but it is/was pretty weak), while New Relic is Cloud-only
    New Relic was initially focused on small businesses and is growing upwards from there, while Splunk is (still) focused on large enterprises

Alternatives:

Both New Relic and Splunk have alternatives:

    For New Relic alternative you could go for AppDynamics, which is powerful but pricey, or Datadog or SPM from us at Sematext, which are much more affordable. Datadog is Cloud-only, while SPM is both Cloud and On Premises.
    For Splunk alternative you could try DIY ELK stack (free software, but not free time/labour/expertise needed) or something like Logsene from Sematext, which gives you Elasticsearch API, Kibana, etc. and can be used in the Cloud or On Premises.


Kibana
Kibana is an open source (Apache Licensed), browser based analytics and search dashboard for Elasticsearch. Kibana is a snap to setup and start using. Kibana strives to be easy to get started with, while also being flexible and powerful, just like Elasticsearch.
Prometheus
Prometheus is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true.
Graphite
Graphite does two things: 1) Store numeric time-series data and 2) Render graphs of this data on demand
Splunk
Splunk Inc. provides the leading platform for Operational Intelligence. Customers use Splunk to search, monitor, analyze and visualize machine data.
NetData
Netdata is distributed, real-time, performance and health monitoring for systems and applications. It is a highly optimized monitoring agent you install on all your systems and containers



