Implementation of Actuator using statsD 
1.Add metrics-actuator dependencies.
2.Add to application properties management.endpoints.web.exposure.include=* to expose all the data metrics at endpoint. 
3.We can create our customized end point by extending predefined.
4.We can define end points using annotations @WriteOperation at put api, @ReadOperation at get api and @DeleteOperation at delete api.
5. Now run your application and check at /actuator/info or /mappings or /metrics or /env to view various actuator data.

Adding Statsd

6.Add Micrometer Configuration : create a bean for meterRegistryCustomizer with meterRegistry .
7.Add @Timed annotation at your controller or at ur api's for different metrics.
@Timed(value="userInfo.gettingAll.request",
		       histogram=true,
		       percentiles = {0.95,0.99},
		       extraTags = {"version","1.0"})
We can add extra tags , histograms to maintain history, percentiles, extra tags, and long task if task is long.
8.Now Add to application properties the statsd port number where u want send statsd data.
management.metrics.export.statsd.port:9125, by default port number is 9125

9.Now after running application we can check actuator/metric and after api call we can see the timed metrics columns getting created.

Adding telegrad and influxdb to store data.
10.We need to download influxdb and telegraf onto our system.
11.Run the telegraf on your system check its status
12 using command sudo systemctl status influxdb
13.now create a database in influxdb with name telegraf and then create a user with password and allow privileges.
14.Now create a telegraf.conf file with a input source statsd and output source influxdb using command :  telegraf -sample-config -input-filter statsd -output-filter influxdb > telegraf.conf

15.Now open telegraf.conf in edit mode using cmd:
gedit  /etc/telegraf/telegraf.conf.

16.Now add the user name and password and database name of ur influxdb database;
17. then close and run ur telegraf using cmd: telegraf -debug
18.Now run ur application ,if there is error u will get to know about it.
19.Now check ur database if data is getting stored.

Now Adding Grafana to view graphically your collected metrics.
20.Download Grafana.
21.run it in localhost:3000
22.check all the ports running using cmd:sudo netstat -tulpn
23.We can kill port using cmd : sudo kill -9 portId
24.Now we open grafana and add the database.
25.Select database and provide basic auth username and password and choose direct connection.
26.After that u can add new Components and panels on ur grafana .
27. Edit the panels and add new queries to make visualize data.



Now Adding Restrictions on Api's
1.Create a @Annotation Throttle which take various inputs like second,minutes,hours and day request.
2.Create Aspect with conditions to check and store the api logs and throw error if logs exceeds limits.
3.Now create a jar of this application.
4.Add dependency in you Api and use it.
5.you need to define all 4 properties in ur application.properties so it dont cause bean creation error.
6.Successfully Run your application.


Now Adding Email Error Sending:
1.Create a microservice to send in your api.Also create restTemplate bean.
2.Add a template in your email api to send.
3.Now To send email at every request exceeds. 
4.Add email api caller using requestTemplate at condition of throttle aspect.


Implementing new Relic
1.Create an account on rpm.newrelic.com.
2.Download the latest java agent.Currently 5.11.0
3.Extract files on to your Resource. Using cmd unzip newrelic-java-5.11.0.zip -d /path/to/resource .
4.Add newrelic agent dependencies in pom file.
5.Edit the newrelif.yml file.
6.Add your license key and application name in newrelif.yml.
7.Now copy the newrelif.jar and newrelif.yml in your application target folder
8.Now run using cmd :java -javaagent:newrelic.jar -jar myapplication.jar 
9.Agent at the newrelic will be created .
10.Now we can view various api logs and visualize api .


Implementing MongoDB.
1.Download MongoDb : sudo apt-get install -y mongodb-org
2.Start and check status of mongoDB using cmd :sudo systemctl start mongod
sudo systemctl enable mongod
3.Now open mongodb using cmd :mongo
4.Add a user with name and password and role, using create user in mongoDB.
5.Now create a model in spring boot and add @Document .
6.Each row in mongoDb is document , each table is a collection, and above that is database.
7.Create a Repository interface and extend it to MongoRepository . Now we can create MongoTemplate and MongoRepository methods based on our requirement.
8.MongoTemplates provide better searching methods and MongoRepository provides CRUD operations.
9.Now Create a service and controller , Add to application properties, MongoDb port,username,password,dbname.
10.spring.data.mongodb.host=localhost
spring.data.mongodb.port=27017
spring.data.mongodb.database=admin
spring.data.mongodb.username=admin
spring.data.mongodb.password=admin
spring.data.mongodb.auto-index-creation=false
11.We can also add it like this, by default database is test. But We can change name and it will be autocreated.
#spring.data.mongodb.uri=mongodb://root:root@localhost:27017/test_db



Implementing aggregation functions

Splunk vs newRelic 
Similarities:

    Initially focused on machine data (metrics for NR, logs for Splunk)
    Initially focused on DevOps, but spreading to other areas - security, marketing, business metrics, and so on
    Both fairly expensive compared to most of their alternatives

Differences:

    New Relic is focused on APM at the moment. Splunk is/was focused on logs and other forms of event data, other than metrics
    Splunk is focused on On Premises deployments (they have/had a Cloud offering, but it is/was pretty weak), while New Relic is Cloud-only
    New Relic was initially focused on small businesses and is growing upwards from there, while Splunk is (still) focused on large enterprises

Alternatives:

Both New Relic and Splunk have alternatives:

    For New Relic alternative you could go for AppDynamics, which is powerful but pricey, or Datadog or SPM from us at Sematext, which are much more affordable. Datadog is Cloud-only, while SPM is both Cloud and On Premises.
    For Splunk alternative you could try DIY ELK stack (free software, but not free time/labour/expertise needed) or something like Logsene from Sematext, which gives you Elasticsearch API, Kibana, etc. and can be used in the Cloud or On Premises.


